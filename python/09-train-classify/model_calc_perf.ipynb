{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76639712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from collections import namedtuple\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea2fc94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.load('best.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a993d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'model_state_dict', 'optimizer_state_dict', 'accuracy', 'loss'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7054f4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torchvision.models.densenet201(num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a8a467f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(state['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30e8314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1b971bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state['optimizer_state_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e2323e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.load('model_211206.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3de3221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = r'/dataset/kerrytj/kerrytj-image/image'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ea4dca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = Path(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f718c691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('/dataset/kerrytj/kerrytj-image/image/bad'), PosixPath('/dataset/kerrytj/kerrytj-image/image/good')]\n",
      "['bad', 'good']\n"
     ]
    }
   ],
   "source": [
    "dirs = sorted(list(folder_path.iterdir()))\n",
    "print(dirs)\n",
    "classes = [i.name for i in dirs]\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11a01e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = [sorted(list(i.iterdir())) for i in dirs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16da8ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff3e1319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction = namedtuple('Prediction', ['path', 'target', 'predicted'])\n",
    "\n",
    "class Prediction:\n",
    "    def __init__(self, path, target, predicted, conf):\n",
    "        self.path = path\n",
    "        self.target = target\n",
    "        self.predicted = predicted\n",
    "        self.conf = conf\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return str((self.path, self.target, self.predicted, self.conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ff85d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [ Prediction(p, i, None, None) for i, paths in enumerate(image_paths) for p in paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4649d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17b2b24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use device cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "print('use device', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "208ee49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b5c3fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.Resize((540, 540)),\n",
    "    T.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7184cfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for p in predictions:\n",
    "    with Image.open(p.path) as im:\n",
    "        input_tensor = transform(im)\n",
    "    input_tensor = input_tensor.unsqueeze(dim=0)\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        result = net(input_tensor)\n",
    "        result = F.softmax(result, dim=1)\n",
    "    result = result.squeeze(dim=0)\n",
    "    conf, index = result.max(dim=0)\n",
    "    index = index.cpu().item()\n",
    "    conf = conf.cpu().item()\n",
    "    \n",
    "    p.predicted = index\n",
    "    p.conf = conf\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f116aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/bad/082407_98204676247_S.JPG'), 0, 1, 0.9982408285140991)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/bad/091717_74162950733_S.JPG'), 0, 1, 0.9737665057182312)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/bad/092405_43666481794_S.JPG'), 0, 1, 0.6395330429077148)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/bad/100333_6239074446_S.JPG'), 0, 1, 0.8435037136077881)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/bad/103905_40350653161_S.JPG'), 0, 1, 0.6401761174201965)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/bad/104935_90716376109_S.JPG'), 0, 1, 0.9015095829963684)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/bad/134603_15884762824_S.JPG'), 0, 1, 0.9847902655601501)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/good/092027_74165340169_S.JPG'), 1, 0, 0.7600827217102051)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/good/123304_40385143393_S.JPG'), 1, 0, 0.8958895206451416)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/good/150236_96205532948_S.JPG'), 1, 0, 0.8044833540916443)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/good/205501_13218530614_S.JPG'), 1, 0, 0.9981702566146851)\n"
     ]
    }
   ],
   "source": [
    "for p in predictions:\n",
    "    if p.target != p.predicted:\n",
    "        print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8016d1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.989065606361829\n"
     ]
    }
   ],
   "source": [
    "miss_count = len(list(filter(lambda p: p.target != p.predicted, predictions)))\n",
    "print(1 - miss_count / len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff6578b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2294895c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.Resize((512, 512)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "78971e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_prediction(net, transform):\n",
    "    predictions = [ Prediction(p, i, None, None) for i, paths in enumerate(image_paths) for p in paths]\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # device = torch.device('cpu')\n",
    "    print('use device', device)\n",
    "    net = net.eval().to(device)\n",
    "    \n",
    "    for p in predictions:\n",
    "        with Image.open(p.path) as im:\n",
    "            input_tensor = transform(im)\n",
    "        input_tensor = input_tensor.unsqueeze(dim=0)\n",
    "        input_tensor = input_tensor.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            result = net(input_tensor)\n",
    "            result = F.softmax(result, dim=1)\n",
    "        result = result.squeeze(dim=0)\n",
    "        conf, index = result.max(dim=0)\n",
    "        index = index.cpu().item()\n",
    "        conf = conf.cpu().item()\n",
    "\n",
    "        p.predicted = index\n",
    "        p.conf = conf\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91f2bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3cce23b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = torch.load('best.pth')\n",
    "net = torchvision.models.densenet201(num_classes=2)\n",
    "net.load_state_dict(state['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e15b5702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use device cuda\n"
     ]
    }
   ],
   "source": [
    "predictions = do_prediction(net, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "399b4b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/bad/082407_98204676247_S.JPG'), 0, 1, 0.9615916013717651)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/bad/092138_74163202404_S.JPG'), 0, 1, 0.6098570227622986)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/bad/092459_13238508352_S.JPG'), 0, 1, 0.6185823082923889)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/bad/094350_92959832555_S.JPG'), 0, 1, 0.6206309199333191)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/good/083755_74163467706_S.JPG'), 1, 0, 0.510867178440094)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/good/085240_43666489548_S.JPG'), 1, 0, 0.8957359194755554)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/good/091239_93874689744_S.JPG'), 1, 0, 0.9700624346733093)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/good/091320_44185199326_S.JPG'), 1, 0, 0.5965411067008972)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/good/092027_74165340169_S.JPG'), 1, 0, 0.8810672760009766)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/good/092825_95820865669_S.JPG'), 1, 0, 0.8170564770698547)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/good/093211_40446478108_S.JPG'), 1, 0, 0.605231523513794)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/good/094021_74163684110_S.JPG'), 1, 0, 0.8759323358535767)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/good/094657_92949151622_S.JPG'), 1, 0, 0.5373110771179199)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/good/095123_91705590867_S.JPG'), 1, 0, 0.6033645272254944)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/good/103839_40385146149_S.JPG'), 1, 0, 0.639661431312561)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/good/104841_40380559406_S.JPG'), 1, 0, 0.8542401194572449)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/good/105842_91795139069_P.JPG'), 1, 0, 0.9999927282333374)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/good/105950_43666508788_S.JPG'), 1, 0, 0.9366861581802368)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/good/110432_15876115134_S.JPG'), 1, 0, 0.850814938545227)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/good/110748_92544430411_S.JPG'), 1, 0, 0.7061573266983032)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/good/114503_40385162086_S.JPG'), 1, 0, 0.6778998374938965)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/good/114558_74162855532_S.JPG'), 1, 0, 0.9929878115653992)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/good/114909_74163782002_S.JPG'), 1, 0, 0.527580738067627)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/good/120538_43697888593_S.JPG'), 1, 0, 0.9279425144195557)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/good/121229_6174542776_S.JPG'), 1, 0, 0.7045254111289978)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/good/122535_6175649281_S.JPG'), 1, 0, 0.5377495288848877)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/good/123304_40385143393_S.JPG'), 1, 0, 0.5654048919677734)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/good/132122_93807908608_S.JPG'), 1, 0, 0.7211853861808777)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/good/133425_6175817653_S.JPG'), 1, 0, 0.7760241031646729)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/good/133458_96507206934_S.JPG'), 1, 0, 0.5876604914665222)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/good/140130_43672326997_S.JPG'), 1, 0, 0.8041236996650696)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/good/143633_15474098355_S.JPG'), 1, 0, 0.5153160095214844)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/good/151255_96862952751_S.JPG'), 1, 0, 0.9097471237182617)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/good/165504_74162907814_S.JPG'), 1, 0, 0.6271213293075562)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/good/182003_40385170195_S.JPG'), 1, 0, 0.514293372631073)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/good/193424_40385158453_S.JPG'), 1, 0, 0.9987491369247437)\n",
      "(PosixPath('/dataset/kerrytj/kerrytj-image/image/good/205501_13218530614_S.JPG'), 1, 0, 0.9999667406082153)\n"
     ]
    }
   ],
   "source": [
    "for p in predictions:\n",
    "    if p.target != p.predicted:\n",
    "        print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2d7dde99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.963220675944334\n"
     ]
    }
   ],
   "source": [
    "miss_count = len(list(filter(lambda p: p.target != p.predicted, predictions)))\n",
    "print(1 - miss_count / len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df1d911",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e417b71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0223b814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c40960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d57a6a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use device cuda\n"
     ]
    }
   ],
   "source": [
    "net = torch.load('model_211206.pt')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "print('use device', device)\n",
    "net = net.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1d97b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.Resize((540, 540)),\n",
    "    T.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d49b4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.41995573043823\n"
     ]
    }
   ],
   "source": [
    "time_accu = 0\n",
    "for p in predictions[:1000]:\n",
    "    with Image.open(p.path) as im:\n",
    "        input_tensor = transform(im)\n",
    "    input_tensor = input_tensor.unsqueeze(dim=0)\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        result = net(input_tensor)\n",
    "        result = F.softmax(result, dim=1)\n",
    "    \n",
    "    result = result.squeeze(dim=0)\n",
    "    conf, index = result.max(dim=0)\n",
    "    index = index.cpu().item()\n",
    "    conf = conf.cpu().item()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    p.predicted = index\n",
    "    p.conf = conf\n",
    "    time_accu += (end_time - start_time)\n",
    "\n",
    "print(time_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb5b620e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = torch.load('best.pth')\n",
    "net = torchvision.models.densenet201(num_classes=2)\n",
    "net.load_state_dict(state['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "36da045b",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9293a7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.Resize((512, 512)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "613104cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269.61998677253723\n"
     ]
    }
   ],
   "source": [
    "time_accu = 0\n",
    "for p in predictions[:1000]:\n",
    "    with Image.open(p.path) as im:\n",
    "        input_tensor = transform(im)\n",
    "    input_tensor = input_tensor.unsqueeze(dim=0)\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        result = net(input_tensor)\n",
    "        result = F.softmax(result, dim=1)\n",
    "    \n",
    "    result = result.squeeze(dim=0)\n",
    "    conf, index = result.max(dim=0)\n",
    "    index = index.cpu().item()\n",
    "    conf = conf.cpu().item()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    p.predicted = index\n",
    "    p.conf = conf\n",
    "    time_accu += (end_time - start_time)\n",
    "\n",
    "print(time_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae13e3e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import random_split, DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, transform):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, n):\n",
    "        data = self.dataset[n]\n",
    "        return self.transform(data[0]), data[1]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = r'C:\\Users\\chtti\\Downloads\\拍照簽收圖檔-20211027T020311Z-001\\拍照簽收圖檔'\n",
    "folder = r'/dataset/kerrytj/kerrytj-image/image'\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.AutoAugment(T.autoaugment.AutoAugmentPolicy.IMAGENET),\n",
    "    T.Resize((540, 540)),\n",
    "    T.ToTensor()\n",
    "])\n",
    "val_transform = T.Compose([\n",
    "    T.Resize((540, 540)),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = ImageFolder(folder)\n",
    "\n",
    "train_length = int(len(dataset) * 0.8)\n",
    "val_length = len(dataset) - train_length\n",
    "train_set, val_set = random_split(dataset, [train_length, val_length], torch.Generator().manual_seed(42))\n",
    "train_set_aug = TransformDataset(train_set, train_transform)\n",
    "val_set_trans = TransformDataset(val_set, val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_set_aug, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          pin_memory=True)\n",
    "val_loader = DataLoader(val_set_trans, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1006"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bad', 'good']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=RGB size=1080x1920 at 0x7F9348A7DFA0>, 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "plt.rcParams['figure.figsize'] = [24, 8]\n",
    "\n",
    "\n",
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = next(iter(train_loader))\n",
    "# grid = make_grid(sample[0])\n",
    "# show(grid)\n",
    "# print(sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = next(iter(val_loader))\n",
    "# print(sample[0].shape)\n",
    "# grid = make_grid(sample[0])\n",
    "# show(grid)\n",
    "# print(sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (7.6.5)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (1.0.2)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (6.3.1)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.1.3)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.5.2)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (7.27.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.0.2)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.4.1)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /opt/conda/lib/python3.8/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (57.4.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (5.0.9)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.18.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (3.0.20)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.2)\n",
      "Requirement already satisfied: pyzmq>=13 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (22.2.1)\n",
      "Requirement already satisfied: jupyter-core>=4.6.0 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (4.7.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (1.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (0.3)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets) (3.2.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (21.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.18.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.8/site-packages (from widgetsnbextension~=3.5.0->ipywidgets) (6.4.1)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (21.1.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.0.1)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.8.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.11.1)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (6.1.0)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.8/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.11.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.20)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.4.3)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (4.1.0)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.4)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (21.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.4.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = torchvision.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torchvision.models.resnet18(num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['fc.weight', 'fc.bias'], unexpected_keys=[])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = resnet.state_dict()\n",
    "del d['fc.weight']\n",
    "del d['fc.bias']\n",
    "\n",
    "net.load_state_dict(d, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "net = net.to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optim = torch.optim.SGD(net.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=20, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0, Loss: 0.764859, TestLoss: 0.587705, Acc: 0.787129\n",
      "Batch: 1, Loss: 0.529260, TestLoss: 0.523980, Acc: 0.841584\n",
      "Batch: 2, Loss: 0.660865, TestLoss: 0.470446, Acc: 0.871287\n",
      "Batch: 3, Loss: 0.293557, TestLoss: 0.427952, Acc: 0.871287\n",
      "Batch: 4, Loss: 0.403436, TestLoss: 0.397893, Acc: 0.876238\n",
      "Batch: 5, Loss: 0.609824, TestLoss: 0.379805, Acc: 0.871287\n",
      "Batch: 6, Loss: 0.458233, TestLoss: 0.362121, Acc: 0.876238\n",
      "Batch: 7, Loss: 0.268159, TestLoss: 0.354705, Acc: 0.861386\n",
      "Batch: 8, Loss: 0.852962, TestLoss: 0.345324, Acc: 0.866337\n",
      "Batch: 9, Loss: 0.513298, TestLoss: 0.328270, Acc: 0.876238\n",
      "Batch: 10, Loss: 0.341506, TestLoss: 0.320410, Acc: 0.881188\n",
      "Batch: 11, Loss: 0.782088, TestLoss: 0.323812, Acc: 0.866337\n",
      "Batch: 12, Loss: 0.217180, TestLoss: 0.305445, Acc: 0.881188\n",
      "Batch: 13, Loss: 0.461835, TestLoss: 0.299070, Acc: 0.886139\n",
      "Batch: 14, Loss: 0.188713, TestLoss: 0.296266, Acc: 0.881188\n",
      "Batch: 15, Loss: 0.194582, TestLoss: 0.295408, Acc: 0.881188\n",
      "Batch: 16, Loss: 0.159632, TestLoss: 0.290308, Acc: 0.891089\n",
      "Batch: 17, Loss: 0.289040, TestLoss: 0.286905, Acc: 0.891089\n",
      "Batch: 18, Loss: 0.229954, TestLoss: 0.282577, Acc: 0.886139\n",
      "Batch: 19, Loss: 0.122801, TestLoss: 0.278164, Acc: 0.886139\n",
      "Batch: 20, Loss: 0.310093, TestLoss: 0.275710, Acc: 0.886139\n",
      "Batch: 21, Loss: 0.101818, TestLoss: 0.277304, Acc: 0.891089\n",
      "Batch: 22, Loss: 0.266086, TestLoss: 0.268272, Acc: 0.886139\n",
      "Batch: 23, Loss: 0.323761, TestLoss: 0.274076, Acc: 0.891089\n",
      "Batch: 24, Loss: 0.779955, TestLoss: 0.263760, Acc: 0.891089\n",
      "Batch: 25, Loss: 0.336313, TestLoss: 0.261443, Acc: 0.891089\n",
      "Batch: 26, Loss: 0.611517, TestLoss: 0.262786, Acc: 0.891089\n",
      "Batch: 27, Loss: 0.152919, TestLoss: 0.264623, Acc: 0.896040\n",
      "Batch: 28, Loss: 0.104847, TestLoss: 0.269313, Acc: 0.891089\n",
      "Batch: 29, Loss: 1.000542, TestLoss: 0.272931, Acc: 0.900990\n",
      "Batch: 30, Loss: 0.066819, TestLoss: 0.257797, Acc: 0.896040\n",
      "Batch: 31, Loss: 0.128589, TestLoss: 0.249847, Acc: 0.905941\n",
      "Batch: 32, Loss: 0.655511, TestLoss: 0.257275, Acc: 0.900990\n",
      "Batch: 33, Loss: 1.209111, TestLoss: 0.255429, Acc: 0.905941\n",
      "Batch: 34, Loss: 0.079716, TestLoss: 0.250994, Acc: 0.905941\n",
      "Batch: 35, Loss: 0.451391, TestLoss: 0.247189, Acc: 0.900990\n",
      "Batch: 36, Loss: 0.460759, TestLoss: 0.246072, Acc: 0.900990\n",
      "Batch: 37, Loss: 0.263111, TestLoss: 0.248068, Acc: 0.905941\n",
      "Batch: 38, Loss: 0.063287, TestLoss: 0.242744, Acc: 0.905941\n",
      "Batch: 39, Loss: 0.118740, TestLoss: 0.246500, Acc: 0.900990\n",
      "Batch: 40, Loss: 0.193007, TestLoss: 0.235021, Acc: 0.910891\n",
      "Batch: 41, Loss: 0.037225, TestLoss: 0.240665, Acc: 0.900990\n",
      "Batch: 42, Loss: 0.313699, TestLoss: 0.238543, Acc: 0.905941\n",
      "Batch: 43, Loss: 0.069625, TestLoss: 0.237788, Acc: 0.900990\n",
      "Batch: 44, Loss: 0.129052, TestLoss: 0.234944, Acc: 0.910891\n",
      "Batch: 45, Loss: 0.528270, TestLoss: 0.230883, Acc: 0.910891\n",
      "Batch: 46, Loss: 0.520025, TestLoss: 0.240798, Acc: 0.900990\n",
      "Batch: 47, Loss: 0.960877, TestLoss: 0.238694, Acc: 0.900990\n",
      "Batch: 48, Loss: 0.071917, TestLoss: 0.232346, Acc: 0.910891\n",
      "Batch: 49, Loss: 0.030810, TestLoss: 0.234598, Acc: 0.900990\n",
      "Batch: 50, Loss: 0.073705, TestLoss: 0.232195, Acc: 0.905941\n",
      "Batch: 51, Loss: 0.088334, TestLoss: 0.228747, Acc: 0.915842\n",
      "Batch: 52, Loss: 0.987047, TestLoss: 0.224626, Acc: 0.915842\n",
      "Batch: 53, Loss: 0.419198, TestLoss: 0.228024, Acc: 0.900990\n",
      "Batch: 54, Loss: 0.438188, TestLoss: 0.232585, Acc: 0.900990\n",
      "Batch: 55, Loss: 0.183493, TestLoss: 0.226437, Acc: 0.905941\n",
      "Batch: 56, Loss: 0.806133, TestLoss: 0.216629, Acc: 0.920792\n",
      "Batch: 57, Loss: 0.743909, TestLoss: 0.221833, Acc: 0.905941\n",
      "Batch: 58, Loss: 0.792402, TestLoss: 0.218139, Acc: 0.920792\n",
      "Batch: 59, Loss: 0.040844, TestLoss: 0.217839, Acc: 0.915842\n",
      "Batch: 60, Loss: 0.146128, TestLoss: 0.222776, Acc: 0.905941\n",
      "Batch: 61, Loss: 0.156493, TestLoss: 0.227858, Acc: 0.905941\n",
      "Batch: 62, Loss: 0.153860, TestLoss: 0.225466, Acc: 0.905941\n",
      "Batch: 63, Loss: 0.093256, TestLoss: 0.212985, Acc: 0.915842\n",
      "Batch: 64, Loss: 0.991744, TestLoss: 0.214467, Acc: 0.920792\n",
      "Batch: 65, Loss: 0.256399, TestLoss: 0.212845, Acc: 0.930693\n",
      "Batch: 66, Loss: 0.243016, TestLoss: 0.218839, Acc: 0.915842\n",
      "Batch: 67, Loss: 0.028683, TestLoss: 0.216143, Acc: 0.910891\n",
      "Batch: 68, Loss: 0.247461, TestLoss: 0.213993, Acc: 0.920792\n",
      "Batch: 69, Loss: 0.146927, TestLoss: 0.214613, Acc: 0.910891\n",
      "Batch: 70, Loss: 0.082389, TestLoss: 0.213535, Acc: 0.920792\n",
      "Batch: 71, Loss: 0.163125, TestLoss: 0.211184, Acc: 0.920792\n",
      "Batch: 72, Loss: 0.105068, TestLoss: 0.213621, Acc: 0.910891\n",
      "Batch: 73, Loss: 0.816883, TestLoss: 0.210701, Acc: 0.920792\n",
      "Batch: 74, Loss: 0.025233, TestLoss: 0.205298, Acc: 0.930693\n",
      "Batch: 75, Loss: 1.442771, TestLoss: 0.207220, Acc: 0.935644\n",
      "Batch: 76, Loss: 0.563174, TestLoss: 0.207973, Acc: 0.930693\n",
      "Batch: 77, Loss: 0.205030, TestLoss: 0.212759, Acc: 0.915842\n",
      "Batch: 78, Loss: 0.112835, TestLoss: 0.205953, Acc: 0.930693\n",
      "Batch: 79, Loss: 0.122298, TestLoss: 0.206170, Acc: 0.930693\n",
      "Batch: 80, Loss: 0.116648, TestLoss: 0.213099, Acc: 0.915842\n",
      "Batch: 81, Loss: 0.023307, TestLoss: 0.204929, Acc: 0.925743\n",
      "Batch: 82, Loss: 0.932588, TestLoss: 0.197491, Acc: 0.935644\n",
      "Batch: 83, Loss: 0.017377, TestLoss: 0.202002, Acc: 0.935644\n",
      "Batch: 84, Loss: 1.382537, TestLoss: 0.198051, Acc: 0.935644\n",
      "Batch: 85, Loss: 0.075235, TestLoss: 0.216366, Acc: 0.910891\n",
      "Batch: 86, Loss: 0.116466, TestLoss: 0.205112, Acc: 0.915842\n",
      "Batch: 87, Loss: 0.313645, TestLoss: 0.200863, Acc: 0.935644\n",
      "Batch: 88, Loss: 0.058551, TestLoss: 0.200697, Acc: 0.925743\n",
      "Batch: 89, Loss: 0.168603, TestLoss: 0.198949, Acc: 0.935644\n",
      "Batch: 90, Loss: 0.053169, TestLoss: 0.192923, Acc: 0.930693\n",
      "Batch: 91, Loss: 0.027276, TestLoss: 0.200422, Acc: 0.930693\n",
      "Batch: 92, Loss: 0.734241, TestLoss: 0.200049, Acc: 0.925743\n",
      "Batch: 93, Loss: 0.061152, TestLoss: 0.200397, Acc: 0.930693\n",
      "Batch: 94, Loss: 0.088600, TestLoss: 0.189162, Acc: 0.935644\n",
      "Batch: 95, Loss: 0.233441, TestLoss: 0.196161, Acc: 0.925743\n",
      "Batch: 96, Loss: 0.163928, TestLoss: 0.213187, Acc: 0.920792\n",
      "Batch: 97, Loss: 0.185064, TestLoss: 0.199832, Acc: 0.925743\n",
      "Batch: 98, Loss: 0.008775, TestLoss: 0.193816, Acc: 0.930693\n",
      "Batch: 99, Loss: 0.567480, TestLoss: 0.204198, Acc: 0.935644\n",
      "Batch: 100, Loss: 1.044101, TestLoss: 0.197358, Acc: 0.940594\n",
      "Batch: 101, Loss: 0.102828, TestLoss: 0.198030, Acc: 0.925743\n",
      "Batch: 102, Loss: 0.097187, TestLoss: 0.197336, Acc: 0.930693\n",
      "Batch: 103, Loss: 0.008293, TestLoss: 0.198642, Acc: 0.935644\n",
      "Batch: 104, Loss: 0.012067, TestLoss: 0.199024, Acc: 0.925743\n",
      "Batch: 105, Loss: 0.245441, TestLoss: 0.192134, Acc: 0.930693\n",
      "Batch: 106, Loss: 1.041493, TestLoss: 0.195541, Acc: 0.935644\n",
      "Batch: 107, Loss: 0.009370, TestLoss: 0.196610, Acc: 0.935644\n",
      "Batch: 108, Loss: 1.097568, TestLoss: 0.194666, Acc: 0.935644\n",
      "Batch: 109, Loss: 1.116865, TestLoss: 0.192006, Acc: 0.940594\n",
      "Batch: 110, Loss: 0.463752, TestLoss: 0.200133, Acc: 0.920792\n",
      "Batch: 111, Loss: 0.300678, TestLoss: 0.191599, Acc: 0.945545\n",
      "Batch: 112, Loss: 0.200024, TestLoss: 0.187850, Acc: 0.940594\n",
      "Batch: 113, Loss: 0.064654, TestLoss: 0.186049, Acc: 0.940594\n",
      "Batch: 114, Loss: 0.129003, TestLoss: 0.191312, Acc: 0.940594\n",
      "Batch: 115, Loss: 0.052811, TestLoss: 0.192270, Acc: 0.940594\n",
      "Batch: 116, Loss: 0.035009, TestLoss: 0.193776, Acc: 0.945545\n",
      "Batch: 117, Loss: 0.073304, TestLoss: 0.194935, Acc: 0.930693\n",
      "Batch: 118, Loss: 0.168175, TestLoss: 0.198287, Acc: 0.930693\n",
      "Batch: 119, Loss: 0.004026, TestLoss: 0.191495, Acc: 0.945545\n",
      "Batch: 120, Loss: 0.023236, TestLoss: 0.186355, Acc: 0.940594\n",
      "Batch: 121, Loss: 0.654077, TestLoss: 0.181687, Acc: 0.940594\n",
      "Batch: 122, Loss: 0.983711, TestLoss: 0.191893, Acc: 0.935644\n",
      "Batch: 123, Loss: 0.177832, TestLoss: 0.182004, Acc: 0.935644\n",
      "Batch: 124, Loss: 0.123196, TestLoss: 0.192729, Acc: 0.940594\n",
      "Batch: 125, Loss: 0.050515, TestLoss: 0.187369, Acc: 0.940594\n",
      "Batch: 126, Loss: 0.011158, TestLoss: 0.191592, Acc: 0.940594\n",
      "Batch: 127, Loss: 0.043468, TestLoss: 0.187941, Acc: 0.940594\n",
      "Batch: 128, Loss: 1.116509, TestLoss: 0.191021, Acc: 0.935644\n",
      "Batch: 129, Loss: 0.013258, TestLoss: 0.196609, Acc: 0.930693\n",
      "Batch: 130, Loss: 0.010152, TestLoss: 0.192027, Acc: 0.940594\n",
      "Batch: 131, Loss: 0.194519, TestLoss: 0.194007, Acc: 0.925743\n",
      "Batch: 132, Loss: 1.281239, TestLoss: 0.189865, Acc: 0.940594\n",
      "Batch: 133, Loss: 0.160491, TestLoss: 0.202300, Acc: 0.930693\n",
      "Batch: 134, Loss: 0.144410, TestLoss: 0.190673, Acc: 0.940594\n",
      "Batch: 135, Loss: 0.020959, TestLoss: 0.199678, Acc: 0.935644\n",
      "Batch: 136, Loss: 0.107377, TestLoss: 0.191091, Acc: 0.945545\n",
      "Batch: 137, Loss: 0.199020, TestLoss: 0.192482, Acc: 0.945545\n",
      "Batch: 138, Loss: 0.141816, TestLoss: 0.196326, Acc: 0.945545\n",
      "Batch: 139, Loss: 0.109949, TestLoss: 0.193225, Acc: 0.940594\n",
      "Batch: 140, Loss: 0.081119, TestLoss: 0.195107, Acc: 0.940594\n",
      "Batch: 141, Loss: 0.080909, TestLoss: 0.192485, Acc: 0.940594\n",
      "Batch: 142, Loss: 0.016369, TestLoss: 0.199652, Acc: 0.940594\n",
      "Epoch   143: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Batch: 143, Loss: 0.035585, TestLoss: 0.189960, Acc: 0.940594\n",
      "Batch: 144, Loss: 0.923002, TestLoss: 0.215527, Acc: 0.920792\n",
      "Batch: 145, Loss: 0.133356, TestLoss: 0.198055, Acc: 0.935644\n",
      "Batch: 146, Loss: 0.897538, TestLoss: 0.204086, Acc: 0.925743\n",
      "Batch: 147, Loss: 0.039424, TestLoss: 0.193849, Acc: 0.940594\n",
      "Batch: 148, Loss: 0.240891, TestLoss: 0.187260, Acc: 0.945545\n",
      "Batch: 149, Loss: 0.982697, TestLoss: 0.205572, Acc: 0.925743\n",
      "Batch: 150, Loss: 0.010212, TestLoss: 0.191979, Acc: 0.940594\n",
      "Batch: 151, Loss: 0.012851, TestLoss: 0.185051, Acc: 0.940594\n",
      "Batch: 152, Loss: 0.167096, TestLoss: 0.196901, Acc: 0.935644\n",
      "Batch: 153, Loss: 0.129737, TestLoss: 0.195396, Acc: 0.940594\n",
      "Batch: 154, Loss: 0.090304, TestLoss: 0.190846, Acc: 0.940594\n",
      "Batch: 155, Loss: 0.097384, TestLoss: 0.188982, Acc: 0.945545\n",
      "Batch: 156, Loss: 0.031406, TestLoss: 0.199571, Acc: 0.935644\n",
      "Batch: 157, Loss: 0.714221, TestLoss: 0.185524, Acc: 0.940594\n",
      "Batch: 158, Loss: 0.043791, TestLoss: 0.186850, Acc: 0.945545\n",
      "Batch: 159, Loss: 0.139502, TestLoss: 0.195133, Acc: 0.940594\n",
      "Batch: 160, Loss: 0.104658, TestLoss: 0.194553, Acc: 0.940594\n",
      "Batch: 161, Loss: 0.228752, TestLoss: 0.202344, Acc: 0.920792\n",
      "Batch: 162, Loss: 0.114312, TestLoss: 0.199123, Acc: 0.940594\n",
      "Batch: 163, Loss: 0.011874, TestLoss: 0.187795, Acc: 0.945545\n",
      "Epoch   164: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Batch: 164, Loss: 0.128043, TestLoss: 0.187992, Acc: 0.945545\n",
      "Batch: 165, Loss: 0.305249, TestLoss: 0.185173, Acc: 0.945545\n",
      "Batch: 166, Loss: 0.089144, TestLoss: 0.202875, Acc: 0.925743\n",
      "Batch: 167, Loss: 1.376633, TestLoss: 0.204164, Acc: 0.925743\n",
      "Batch: 168, Loss: 0.208848, TestLoss: 0.190020, Acc: 0.945545\n",
      "Batch: 169, Loss: 0.168111, TestLoss: 0.191517, Acc: 0.935644\n",
      "Batch: 170, Loss: 1.235115, TestLoss: 0.203556, Acc: 0.930693\n",
      "Batch: 171, Loss: 0.093996, TestLoss: 0.188926, Acc: 0.945545\n",
      "Batch: 172, Loss: 0.085234, TestLoss: 0.191464, Acc: 0.940594\n",
      "Batch: 173, Loss: 0.038161, TestLoss: 0.188021, Acc: 0.945545\n",
      "Batch: 174, Loss: 0.290458, TestLoss: 0.203233, Acc: 0.920792\n",
      "Batch: 175, Loss: 0.217803, TestLoss: 0.207696, Acc: 0.920792\n",
      "Batch: 176, Loss: 0.107105, TestLoss: 0.196971, Acc: 0.940594\n",
      "Batch: 177, Loss: 0.004108, TestLoss: 0.193402, Acc: 0.940594\n",
      "Batch: 178, Loss: 0.073661, TestLoss: 0.192826, Acc: 0.940594\n",
      "Batch: 179, Loss: 0.524977, TestLoss: 0.213606, Acc: 0.920792\n",
      "Batch: 180, Loss: 0.008267, TestLoss: 0.190616, Acc: 0.945545\n",
      "Batch: 181, Loss: 0.010952, TestLoss: 0.185879, Acc: 0.945545\n",
      "Batch: 182, Loss: 0.158779, TestLoss: 0.188679, Acc: 0.935644\n",
      "Batch: 183, Loss: 0.604650, TestLoss: 0.194417, Acc: 0.940594\n",
      "Batch: 184, Loss: 0.044358, TestLoss: 0.188997, Acc: 0.945545\n",
      "Epoch   185: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Batch: 185, Loss: 0.093756, TestLoss: 0.197816, Acc: 0.940594\n",
      "Batch: 186, Loss: 0.004260, TestLoss: 0.193203, Acc: 0.940594\n",
      "Batch: 187, Loss: 0.512814, TestLoss: 0.193776, Acc: 0.935644\n",
      "Batch: 188, Loss: 0.003272, TestLoss: 0.188400, Acc: 0.945545\n",
      "Batch: 189, Loss: 0.171251, TestLoss: 0.198158, Acc: 0.940594\n",
      "Batch: 190, Loss: 0.122148, TestLoss: 0.212428, Acc: 0.920792\n",
      "Batch: 191, Loss: 0.125280, TestLoss: 0.204949, Acc: 0.930693\n",
      "Batch: 192, Loss: 1.233901, TestLoss: 0.216453, Acc: 0.920792\n",
      "Batch: 193, Loss: 0.409399, TestLoss: 0.191986, Acc: 0.940594\n",
      "Batch: 194, Loss: 0.155219, TestLoss: 0.189971, Acc: 0.940594\n",
      "Batch: 195, Loss: 0.256929, TestLoss: 0.190740, Acc: 0.940594\n",
      "Batch: 196, Loss: 1.271865, TestLoss: 0.201543, Acc: 0.920792\n",
      "Batch: 197, Loss: 0.072593, TestLoss: 0.194343, Acc: 0.940594\n",
      "Batch: 198, Loss: 0.313725, TestLoss: 0.198246, Acc: 0.940594\n",
      "Batch: 199, Loss: 0.304438, TestLoss: 0.189170, Acc: 0.945545\n",
      "Batch: 200, Loss: 1.391599, TestLoss: 0.185507, Acc: 0.940594\n",
      "Batch: 201, Loss: 0.038106, TestLoss: 0.184314, Acc: 0.945545\n",
      "Batch: 202, Loss: 0.005455, TestLoss: 0.186808, Acc: 0.945545\n",
      "Batch: 203, Loss: 0.792470, TestLoss: 0.189460, Acc: 0.950495\n",
      "Batch: 204, Loss: 0.071944, TestLoss: 0.195674, Acc: 0.940594\n",
      "Batch: 205, Loss: 0.264892, TestLoss: 0.186295, Acc: 0.940594\n",
      "Epoch   206: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Batch: 206, Loss: 0.058659, TestLoss: 0.203072, Acc: 0.940594\n",
      "Batch: 207, Loss: 0.206041, TestLoss: 0.198254, Acc: 0.930693\n",
      "Batch: 208, Loss: 0.270319, TestLoss: 0.203218, Acc: 0.925743\n",
      "Batch: 209, Loss: 0.097189, TestLoss: 0.191231, Acc: 0.935644\n",
      "Batch: 210, Loss: 0.002693, TestLoss: 0.182932, Acc: 0.945545\n",
      "Batch: 211, Loss: 1.031358, TestLoss: 0.188015, Acc: 0.940594\n",
      "Batch: 212, Loss: 0.165445, TestLoss: 0.187361, Acc: 0.945545\n",
      "Batch: 213, Loss: 0.503255, TestLoss: 0.192524, Acc: 0.940594\n",
      "Batch: 214, Loss: 0.162058, TestLoss: 0.206309, Acc: 0.930693\n",
      "Batch: 215, Loss: 0.068237, TestLoss: 0.189916, Acc: 0.940594\n",
      "Batch: 216, Loss: 0.081489, TestLoss: 0.200115, Acc: 0.930693\n",
      "Batch: 217, Loss: 0.011080, TestLoss: 0.189361, Acc: 0.940594\n",
      "Batch: 218, Loss: 0.183815, TestLoss: 0.197521, Acc: 0.940594\n",
      "Batch: 219, Loss: 0.549857, TestLoss: 0.187865, Acc: 0.945545\n",
      "Batch: 220, Loss: 0.080511, TestLoss: 0.201522, Acc: 0.935644\n",
      "Batch: 221, Loss: 0.076953, TestLoss: 0.193424, Acc: 0.935644\n",
      "Batch: 222, Loss: 0.200666, TestLoss: 0.193080, Acc: 0.940594\n",
      "Batch: 223, Loss: 0.458840, TestLoss: 0.198316, Acc: 0.935644\n",
      "Batch: 224, Loss: 1.020914, TestLoss: 0.207404, Acc: 0.925743\n",
      "Batch: 225, Loss: 0.104896, TestLoss: 0.189895, Acc: 0.940594\n",
      "Batch: 226, Loss: 1.064536, TestLoss: 0.204203, Acc: 0.920792\n",
      "Epoch   227: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Batch: 227, Loss: 0.013741, TestLoss: 0.190361, Acc: 0.945545\n",
      "Batch: 228, Loss: 1.287652, TestLoss: 0.192573, Acc: 0.935644\n",
      "Batch: 229, Loss: 0.032340, TestLoss: 0.190789, Acc: 0.940594\n",
      "Batch: 230, Loss: 0.018517, TestLoss: 0.190112, Acc: 0.935644\n",
      "Batch: 231, Loss: 0.887199, TestLoss: 0.188599, Acc: 0.945545\n",
      "Batch: 232, Loss: 0.074332, TestLoss: 0.203898, Acc: 0.920792\n",
      "Batch: 233, Loss: 0.031801, TestLoss: 0.187870, Acc: 0.940594\n",
      "Batch: 234, Loss: 0.071863, TestLoss: 0.202668, Acc: 0.935644\n",
      "Batch: 235, Loss: 0.020297, TestLoss: 0.189886, Acc: 0.945545\n",
      "Batch: 236, Loss: 0.308089, TestLoss: 0.184153, Acc: 0.950495\n",
      "Batch: 237, Loss: 0.292556, TestLoss: 0.193053, Acc: 0.940594\n",
      "Batch: 238, Loss: 0.044960, TestLoss: 0.189546, Acc: 0.940594\n",
      "Batch: 239, Loss: 0.012703, TestLoss: 0.185975, Acc: 0.945545\n",
      "Batch: 240, Loss: 0.198094, TestLoss: 0.190844, Acc: 0.940594\n",
      "Batch: 241, Loss: 0.172382, TestLoss: 0.201181, Acc: 0.940594\n",
      "Batch: 242, Loss: 0.008984, TestLoss: 0.199250, Acc: 0.930693\n",
      "Batch: 243, Loss: 0.013198, TestLoss: 0.189909, Acc: 0.940594\n",
      "Batch: 244, Loss: 0.022559, TestLoss: 0.186951, Acc: 0.945545\n",
      "Batch: 245, Loss: 0.145761, TestLoss: 0.189211, Acc: 0.940594\n",
      "Batch: 246, Loss: 0.058617, TestLoss: 0.187328, Acc: 0.945545\n",
      "Batch: 247, Loss: 0.135233, TestLoss: 0.191481, Acc: 0.940594\n",
      "Batch: 248, Loss: 0.007088, TestLoss: 0.193396, Acc: 0.935644\n",
      "Batch: 249, Loss: 0.096882, TestLoss: 0.200940, Acc: 0.940594\n",
      "Batch: 250, Loss: 0.649032, TestLoss: 0.187138, Acc: 0.940594\n",
      "Batch: 251, Loss: 0.034797, TestLoss: 0.195027, Acc: 0.940594\n",
      "Batch: 252, Loss: 0.086064, TestLoss: 0.197560, Acc: 0.940594\n",
      "Batch: 253, Loss: 0.130749, TestLoss: 0.207349, Acc: 0.930693\n",
      "Batch: 254, Loss: 0.516547, TestLoss: 0.193585, Acc: 0.940594\n",
      "Batch: 255, Loss: 0.003749, TestLoss: 0.187085, Acc: 0.940594\n",
      "Batch: 256, Loss: 0.009988, TestLoss: 0.193478, Acc: 0.940594\n",
      "Batch: 257, Loss: 0.083959, TestLoss: 0.190269, Acc: 0.945545\n",
      "Batch: 258, Loss: 0.197625, TestLoss: 0.187682, Acc: 0.940594\n",
      "Batch: 259, Loss: 0.002979, TestLoss: 0.191751, Acc: 0.940594\n",
      "Batch: 260, Loss: 1.178918, TestLoss: 0.187455, Acc: 0.940594\n",
      "Batch: 261, Loss: 0.019945, TestLoss: 0.190005, Acc: 0.940594\n",
      "Batch: 262, Loss: 0.030362, TestLoss: 0.188083, Acc: 0.945545\n",
      "Batch: 263, Loss: 0.028158, TestLoss: 0.187361, Acc: 0.945545\n",
      "Batch: 264, Loss: 0.003453, TestLoss: 0.195478, Acc: 0.940594\n",
      "Batch: 265, Loss: 0.048080, TestLoss: 0.203826, Acc: 0.935644\n",
      "Batch: 266, Loss: 0.037073, TestLoss: 0.196219, Acc: 0.940594\n",
      "Batch: 267, Loss: 0.180465, TestLoss: 0.193347, Acc: 0.940594\n",
      "Batch: 268, Loss: 0.751812, TestLoss: 0.192906, Acc: 0.940594\n",
      "Batch: 269, Loss: 0.012222, TestLoss: 0.189701, Acc: 0.945545\n",
      "Batch: 270, Loss: 0.021782, TestLoss: 0.195361, Acc: 0.945545\n",
      "Batch: 271, Loss: 0.014329, TestLoss: 0.189510, Acc: 0.945545\n",
      "Batch: 272, Loss: 0.175393, TestLoss: 0.193630, Acc: 0.945545\n",
      "Batch: 273, Loss: 0.004161, TestLoss: 0.195234, Acc: 0.940594\n",
      "Batch: 274, Loss: 0.821768, TestLoss: 0.205319, Acc: 0.920792\n",
      "Batch: 275, Loss: 0.019445, TestLoss: 0.199750, Acc: 0.940594\n",
      "Batch: 276, Loss: 0.043040, TestLoss: 0.194249, Acc: 0.940594\n",
      "Batch: 277, Loss: 0.010090, TestLoss: 0.194173, Acc: 0.940594\n",
      "Batch: 278, Loss: 0.103324, TestLoss: 0.195696, Acc: 0.940594\n",
      "Batch: 279, Loss: 1.102737, TestLoss: 0.210177, Acc: 0.920792\n",
      "Batch: 280, Loss: 1.101732, TestLoss: 0.210089, Acc: 0.920792\n",
      "Batch: 281, Loss: 0.123382, TestLoss: 0.192688, Acc: 0.940594\n",
      "Batch: 282, Loss: 0.015947, TestLoss: 0.190514, Acc: 0.940594\n",
      "Batch: 283, Loss: 0.001392, TestLoss: 0.193478, Acc: 0.940594\n",
      "Batch: 284, Loss: 0.241754, TestLoss: 0.187518, Acc: 0.945545\n",
      "Batch: 285, Loss: 0.060546, TestLoss: 0.189983, Acc: 0.945545\n",
      "Batch: 286, Loss: 1.230891, TestLoss: 0.190203, Acc: 0.940594\n",
      "Batch: 287, Loss: 0.130764, TestLoss: 0.194790, Acc: 0.935644\n",
      "Batch: 288, Loss: 0.102129, TestLoss: 0.185985, Acc: 0.940594\n",
      "Batch: 289, Loss: 0.906682, TestLoss: 0.220230, Acc: 0.920792\n",
      "Batch: 290, Loss: 0.579200, TestLoss: 0.188894, Acc: 0.945545\n",
      "Batch: 291, Loss: 0.119262, TestLoss: 0.195642, Acc: 0.940594\n",
      "Batch: 292, Loss: 0.008271, TestLoss: 0.190881, Acc: 0.940594\n",
      "Batch: 293, Loss: 0.192173, TestLoss: 0.189875, Acc: 0.945545\n",
      "Batch: 294, Loss: 0.052709, TestLoss: 0.200062, Acc: 0.930693\n",
      "Batch: 295, Loss: 0.145502, TestLoss: 0.187803, Acc: 0.940594\n",
      "Batch: 296, Loss: 0.146930, TestLoss: 0.195642, Acc: 0.940594\n",
      "Batch: 297, Loss: 0.447826, TestLoss: 0.192489, Acc: 0.945545\n",
      "Batch: 298, Loss: 0.076349, TestLoss: 0.202196, Acc: 0.920792\n",
      "Batch: 299, Loss: 0.121125, TestLoss: 0.196397, Acc: 0.945545\n",
      "Batch: 300, Loss: 1.102377, TestLoss: 0.185603, Acc: 0.945545\n",
      "Batch: 301, Loss: 0.052089, TestLoss: 0.188742, Acc: 0.945545\n",
      "Batch: 302, Loss: 0.494319, TestLoss: 0.192070, Acc: 0.940594\n",
      "Batch: 303, Loss: 0.054507, TestLoss: 0.200216, Acc: 0.940594\n",
      "Batch: 304, Loss: 0.049579, TestLoss: 0.189921, Acc: 0.940594\n",
      "Batch: 305, Loss: 0.016532, TestLoss: 0.188780, Acc: 0.945545\n",
      "Batch: 306, Loss: 0.005124, TestLoss: 0.187130, Acc: 0.945545\n",
      "Batch: 307, Loss: 0.194031, TestLoss: 0.211410, Acc: 0.930693\n",
      "Batch: 308, Loss: 0.002485, TestLoss: 0.194429, Acc: 0.940594\n",
      "Batch: 309, Loss: 0.064551, TestLoss: 0.187957, Acc: 0.940594\n",
      "Batch: 310, Loss: 0.028330, TestLoss: 0.193165, Acc: 0.940594\n",
      "Batch: 311, Loss: 0.244330, TestLoss: 0.198888, Acc: 0.935644\n",
      "Batch: 312, Loss: 0.010007, TestLoss: 0.199490, Acc: 0.940594\n",
      "Batch: 313, Loss: 0.003806, TestLoss: 0.187088, Acc: 0.945545\n",
      "Batch: 314, Loss: 0.021068, TestLoss: 0.194493, Acc: 0.940594\n",
      "Batch: 315, Loss: 0.006465, TestLoss: 0.191566, Acc: 0.935644\n",
      "Batch: 316, Loss: 0.044890, TestLoss: 0.207779, Acc: 0.930693\n",
      "Batch: 317, Loss: 0.187542, TestLoss: 0.187631, Acc: 0.940594\n",
      "Batch: 318, Loss: 1.056832, TestLoss: 0.208952, Acc: 0.925743\n",
      "Batch: 319, Loss: 0.003536, TestLoss: 0.196116, Acc: 0.940594\n",
      "Batch: 320, Loss: 1.129547, TestLoss: 0.215619, Acc: 0.920792\n",
      "Batch: 321, Loss: 0.841340, TestLoss: 0.207999, Acc: 0.920792\n",
      "Batch: 322, Loss: 0.060854, TestLoss: 0.196621, Acc: 0.945545\n",
      "Batch: 323, Loss: 0.045327, TestLoss: 0.187493, Acc: 0.945545\n",
      "Batch: 324, Loss: 0.033010, TestLoss: 0.186574, Acc: 0.945545\n",
      "Batch: 325, Loss: 0.004015, TestLoss: 0.194374, Acc: 0.940594\n",
      "Batch: 326, Loss: 0.001571, TestLoss: 0.190612, Acc: 0.940594\n",
      "Batch: 327, Loss: 0.350810, TestLoss: 0.188542, Acc: 0.945545\n",
      "Batch: 328, Loss: 0.916799, TestLoss: 0.188301, Acc: 0.945545\n",
      "Batch: 329, Loss: 0.068092, TestLoss: 0.197040, Acc: 0.940594\n",
      "Batch: 330, Loss: 0.071039, TestLoss: 0.186808, Acc: 0.945545\n",
      "Batch: 331, Loss: 0.085599, TestLoss: 0.195511, Acc: 0.945545\n",
      "Batch: 332, Loss: 0.021161, TestLoss: 0.190162, Acc: 0.940594\n",
      "Batch: 333, Loss: 0.053331, TestLoss: 0.192819, Acc: 0.935644\n",
      "Batch: 334, Loss: 0.217096, TestLoss: 0.192572, Acc: 0.940594\n",
      "Batch: 335, Loss: 0.154363, TestLoss: 0.190061, Acc: 0.945545\n",
      "Batch: 336, Loss: 0.083477, TestLoss: 0.194968, Acc: 0.940594\n",
      "Batch: 337, Loss: 0.123147, TestLoss: 0.201335, Acc: 0.930693\n",
      "Batch: 338, Loss: 0.162577, TestLoss: 0.199509, Acc: 0.940594\n",
      "Batch: 339, Loss: 0.055210, TestLoss: 0.197440, Acc: 0.940594\n",
      "Batch: 340, Loss: 0.077886, TestLoss: 0.187720, Acc: 0.945545\n",
      "Batch: 341, Loss: 0.242357, TestLoss: 0.192146, Acc: 0.945545\n",
      "Batch: 342, Loss: 0.538540, TestLoss: 0.188974, Acc: 0.945545\n",
      "Batch: 343, Loss: 0.062462, TestLoss: 0.183569, Acc: 0.940594\n",
      "Batch: 344, Loss: 0.030890, TestLoss: 0.189568, Acc: 0.945545\n",
      "Batch: 345, Loss: 1.185016, TestLoss: 0.202903, Acc: 0.935644\n",
      "Batch: 346, Loss: 0.829080, TestLoss: 0.188105, Acc: 0.945545\n",
      "Batch: 347, Loss: 0.005898, TestLoss: 0.189578, Acc: 0.940594\n",
      "Batch: 348, Loss: 0.039506, TestLoss: 0.184875, Acc: 0.950495\n",
      "Batch: 349, Loss: 0.115261, TestLoss: 0.191119, Acc: 0.940594\n",
      "Batch: 350, Loss: 0.452926, TestLoss: 0.204541, Acc: 0.930693\n",
      "Batch: 351, Loss: 1.743107, TestLoss: 0.186513, Acc: 0.945545\n",
      "Batch: 352, Loss: 0.778083, TestLoss: 0.189751, Acc: 0.940594\n",
      "Batch: 353, Loss: 0.245660, TestLoss: 0.186998, Acc: 0.945545\n",
      "Batch: 354, Loss: 0.966052, TestLoss: 0.185083, Acc: 0.940594\n",
      "Batch: 355, Loss: 0.042435, TestLoss: 0.192116, Acc: 0.940594\n",
      "Batch: 356, Loss: 1.672305, TestLoss: 0.211669, Acc: 0.920792\n",
      "Batch: 357, Loss: 0.351888, TestLoss: 0.196053, Acc: 0.940594\n",
      "Batch: 358, Loss: 0.856519, TestLoss: 0.188271, Acc: 0.945545\n",
      "Batch: 359, Loss: 0.088872, TestLoss: 0.189641, Acc: 0.945545\n",
      "Batch: 360, Loss: 0.143996, TestLoss: 0.199433, Acc: 0.940594\n",
      "Batch: 361, Loss: 0.271257, TestLoss: 0.189691, Acc: 0.945545\n",
      "Batch: 362, Loss: 1.140917, TestLoss: 0.187426, Acc: 0.940594\n",
      "Batch: 363, Loss: 0.065361, TestLoss: 0.208972, Acc: 0.920792\n",
      "Batch: 364, Loss: 0.043481, TestLoss: 0.213712, Acc: 0.920792\n",
      "Batch: 365, Loss: 1.089522, TestLoss: 0.207235, Acc: 0.920792\n",
      "Batch: 366, Loss: 0.004473, TestLoss: 0.198146, Acc: 0.940594\n",
      "Batch: 367, Loss: 0.026229, TestLoss: 0.194789, Acc: 0.940594\n",
      "Batch: 368, Loss: 1.460611, TestLoss: 0.212099, Acc: 0.920792\n",
      "Batch: 369, Loss: 0.176491, TestLoss: 0.196238, Acc: 0.935644\n",
      "Batch: 370, Loss: 0.279787, TestLoss: 0.192672, Acc: 0.940594\n",
      "Batch: 371, Loss: 0.639345, TestLoss: 0.216101, Acc: 0.920792\n",
      "Batch: 372, Loss: 0.024844, TestLoss: 0.206198, Acc: 0.925743\n",
      "Batch: 373, Loss: 0.055478, TestLoss: 0.192735, Acc: 0.935644\n",
      "Batch: 374, Loss: 0.192369, TestLoss: 0.207344, Acc: 0.925743\n",
      "Batch: 375, Loss: 0.003938, TestLoss: 0.195780, Acc: 0.940594\n",
      "Batch: 376, Loss: 0.027681, TestLoss: 0.189254, Acc: 0.940594\n",
      "Batch: 377, Loss: 0.206202, TestLoss: 0.190096, Acc: 0.940594\n",
      "Batch: 378, Loss: 1.312866, TestLoss: 0.186302, Acc: 0.940594\n",
      "Batch: 379, Loss: 0.048522, TestLoss: 0.196058, Acc: 0.940594\n",
      "Batch: 380, Loss: 0.076536, TestLoss: 0.190124, Acc: 0.945545\n",
      "Batch: 381, Loss: 0.036935, TestLoss: 0.197019, Acc: 0.940594\n",
      "Batch: 382, Loss: 0.002716, TestLoss: 0.191296, Acc: 0.940594\n",
      "Batch: 383, Loss: 0.003313, TestLoss: 0.191755, Acc: 0.940594\n",
      "Batch: 384, Loss: 0.070697, TestLoss: 0.193124, Acc: 0.940594\n",
      "Batch: 385, Loss: 0.301443, TestLoss: 0.205490, Acc: 0.935644\n",
      "Batch: 386, Loss: 2.195118, TestLoss: 0.201771, Acc: 0.930693\n",
      "Batch: 387, Loss: 0.028245, TestLoss: 0.200677, Acc: 0.925743\n",
      "Batch: 388, Loss: 0.086783, TestLoss: 0.189420, Acc: 0.940594\n",
      "Batch: 389, Loss: 0.030468, TestLoss: 0.193547, Acc: 0.940594\n",
      "Batch: 390, Loss: 0.002315, TestLoss: 0.183754, Acc: 0.945545\n",
      "Batch: 391, Loss: 0.002042, TestLoss: 0.194248, Acc: 0.940594\n",
      "Batch: 392, Loss: 0.004639, TestLoss: 0.189849, Acc: 0.940594\n",
      "Batch: 393, Loss: 0.002814, TestLoss: 0.193698, Acc: 0.935644\n",
      "Batch: 394, Loss: 1.109855, TestLoss: 0.203510, Acc: 0.930693\n",
      "Batch: 395, Loss: 0.219786, TestLoss: 0.197990, Acc: 0.935644\n",
      "Batch: 396, Loss: 0.126283, TestLoss: 0.200404, Acc: 0.935644\n",
      "Batch: 397, Loss: 0.053417, TestLoss: 0.185173, Acc: 0.945545\n",
      "Batch: 398, Loss: 0.006921, TestLoss: 0.197366, Acc: 0.940594\n",
      "Batch: 399, Loss: 0.003162, TestLoss: 0.186964, Acc: 0.945545\n",
      "Batch: 400, Loss: 0.011017, TestLoss: 0.194845, Acc: 0.940594\n",
      "Batch: 401, Loss: 0.145890, TestLoss: 0.186737, Acc: 0.940594\n",
      "Batch: 402, Loss: 0.145638, TestLoss: 0.190974, Acc: 0.940594\n",
      "Batch: 403, Loss: 0.045281, TestLoss: 0.203091, Acc: 0.940594\n",
      "Batch: 404, Loss: 0.021291, TestLoss: 0.196924, Acc: 0.940594\n",
      "Batch: 405, Loss: 0.027113, TestLoss: 0.194438, Acc: 0.935644\n",
      "Batch: 406, Loss: 0.008140, TestLoss: 0.206319, Acc: 0.925743\n",
      "Batch: 407, Loss: 0.099879, TestLoss: 0.202272, Acc: 0.930693\n",
      "Batch: 408, Loss: 0.046045, TestLoss: 0.204382, Acc: 0.920792\n",
      "Batch: 409, Loss: 0.274227, TestLoss: 0.191073, Acc: 0.940594\n",
      "Batch: 410, Loss: 1.030434, TestLoss: 0.215696, Acc: 0.920792\n",
      "Batch: 411, Loss: 1.349975, TestLoss: 0.197706, Acc: 0.940594\n",
      "Batch: 412, Loss: 0.292530, TestLoss: 0.186813, Acc: 0.945545\n",
      "Batch: 413, Loss: 0.062259, TestLoss: 0.196543, Acc: 0.940594\n",
      "Batch: 414, Loss: 0.059327, TestLoss: 0.197033, Acc: 0.940594\n",
      "Batch: 415, Loss: 0.108673, TestLoss: 0.194382, Acc: 0.940594\n",
      "Batch: 416, Loss: 0.403688, TestLoss: 0.204088, Acc: 0.925743\n",
      "Batch: 417, Loss: 0.136079, TestLoss: 0.193379, Acc: 0.945545\n",
      "Batch: 418, Loss: 0.003486, TestLoss: 0.185102, Acc: 0.945545\n",
      "Batch: 419, Loss: 0.051226, TestLoss: 0.191468, Acc: 0.940594\n",
      "Batch: 420, Loss: 0.130948, TestLoss: 0.206143, Acc: 0.920792\n",
      "Batch: 421, Loss: 0.665843, TestLoss: 0.183035, Acc: 0.940594\n",
      "Batch: 422, Loss: 0.012875, TestLoss: 0.192128, Acc: 0.935644\n",
      "Batch: 423, Loss: 0.056204, TestLoss: 0.185847, Acc: 0.940594\n",
      "Batch: 424, Loss: 0.007999, TestLoss: 0.191438, Acc: 0.945545\n",
      "Batch: 425, Loss: 0.495086, TestLoss: 0.188569, Acc: 0.945545\n",
      "Batch: 426, Loss: 0.065669, TestLoss: 0.194092, Acc: 0.940594\n",
      "Batch: 427, Loss: 0.038372, TestLoss: 0.194096, Acc: 0.935644\n",
      "Batch: 428, Loss: 0.071772, TestLoss: 0.196462, Acc: 0.935644\n",
      "Batch: 429, Loss: 0.064393, TestLoss: 0.186382, Acc: 0.945545\n",
      "Batch: 430, Loss: 0.109483, TestLoss: 0.185368, Acc: 0.945545\n",
      "Batch: 431, Loss: 0.008153, TestLoss: 0.188214, Acc: 0.940594\n",
      "Batch: 432, Loss: 0.026756, TestLoss: 0.194703, Acc: 0.940594\n",
      "Batch: 433, Loss: 0.147012, TestLoss: 0.197825, Acc: 0.940594\n",
      "Batch: 434, Loss: 0.068711, TestLoss: 0.195750, Acc: 0.935644\n",
      "Batch: 435, Loss: 0.164497, TestLoss: 0.191624, Acc: 0.945545\n",
      "Batch: 436, Loss: 0.110277, TestLoss: 0.190231, Acc: 0.940594\n",
      "Batch: 437, Loss: 0.031972, TestLoss: 0.195441, Acc: 0.945545\n",
      "Batch: 438, Loss: 0.088013, TestLoss: 0.197195, Acc: 0.945545\n",
      "Batch: 439, Loss: 0.181566, TestLoss: 0.206326, Acc: 0.920792\n",
      "Batch: 440, Loss: 0.113982, TestLoss: 0.196508, Acc: 0.940594\n",
      "Batch: 441, Loss: 0.110276, TestLoss: 0.206541, Acc: 0.920792\n",
      "Batch: 442, Loss: 0.202444, TestLoss: 0.196703, Acc: 0.940594\n",
      "Batch: 443, Loss: 0.155423, TestLoss: 0.186998, Acc: 0.945545\n",
      "Batch: 444, Loss: 0.264193, TestLoss: 0.198566, Acc: 0.940594\n",
      "Batch: 445, Loss: 0.047854, TestLoss: 0.190016, Acc: 0.940594\n",
      "Batch: 446, Loss: 0.325951, TestLoss: 0.184850, Acc: 0.945545\n",
      "Batch: 447, Loss: 0.811594, TestLoss: 0.198215, Acc: 0.940594\n",
      "Batch: 448, Loss: 0.041807, TestLoss: 0.203312, Acc: 0.920792\n",
      "Batch: 449, Loss: 0.001618, TestLoss: 0.193106, Acc: 0.940594\n",
      "Batch: 450, Loss: 0.388257, TestLoss: 0.191196, Acc: 0.935644\n",
      "Batch: 451, Loss: 0.061052, TestLoss: 0.198024, Acc: 0.945545\n",
      "Batch: 452, Loss: 0.053771, TestLoss: 0.196021, Acc: 0.935644\n",
      "Batch: 453, Loss: 0.059323, TestLoss: 0.193894, Acc: 0.940594\n",
      "Batch: 454, Loss: 0.004658, TestLoss: 0.196916, Acc: 0.935644\n",
      "Batch: 455, Loss: 0.004143, TestLoss: 0.194896, Acc: 0.940594\n",
      "Batch: 456, Loss: 0.030432, TestLoss: 0.199777, Acc: 0.935644\n",
      "Batch: 457, Loss: 0.016222, TestLoss: 0.196439, Acc: 0.935644\n",
      "Batch: 458, Loss: 0.069854, TestLoss: 0.203177, Acc: 0.925743\n",
      "Batch: 459, Loss: 0.075705, TestLoss: 0.191125, Acc: 0.940594\n",
      "Batch: 460, Loss: 0.032852, TestLoss: 0.195487, Acc: 0.935644\n",
      "Batch: 461, Loss: 0.453438, TestLoss: 0.186668, Acc: 0.940594\n",
      "Batch: 462, Loss: 0.087701, TestLoss: 0.197784, Acc: 0.930693\n",
      "Batch: 463, Loss: 0.077811, TestLoss: 0.205970, Acc: 0.925743\n",
      "Batch: 464, Loss: 0.034023, TestLoss: 0.192088, Acc: 0.940594\n",
      "Batch: 465, Loss: 0.033921, TestLoss: 0.193106, Acc: 0.935644\n",
      "Batch: 466, Loss: 0.131424, TestLoss: 0.193346, Acc: 0.940594\n",
      "Batch: 467, Loss: 0.009948, TestLoss: 0.189060, Acc: 0.945545\n",
      "Batch: 468, Loss: 0.029986, TestLoss: 0.187046, Acc: 0.945545\n",
      "Batch: 469, Loss: 0.025339, TestLoss: 0.196296, Acc: 0.940594\n",
      "Batch: 470, Loss: 0.112175, TestLoss: 0.196913, Acc: 0.940594\n",
      "Batch: 471, Loss: 0.249613, TestLoss: 0.189099, Acc: 0.945545\n",
      "Batch: 472, Loss: 0.402133, TestLoss: 0.187943, Acc: 0.945545\n",
      "Batch: 473, Loss: 0.298220, TestLoss: 0.185892, Acc: 0.940594\n",
      "Batch: 474, Loss: 0.051325, TestLoss: 0.198160, Acc: 0.940594\n",
      "Batch: 475, Loss: 0.225925, TestLoss: 0.186895, Acc: 0.945545\n",
      "Batch: 476, Loss: 0.278154, TestLoss: 0.189754, Acc: 0.940594\n",
      "Batch: 477, Loss: 0.236164, TestLoss: 0.188148, Acc: 0.945545\n",
      "Batch: 478, Loss: 0.005441, TestLoss: 0.186277, Acc: 0.945545\n",
      "Batch: 479, Loss: 1.701352, TestLoss: 0.187576, Acc: 0.945545\n",
      "Batch: 480, Loss: 0.085959, TestLoss: 0.192051, Acc: 0.940594\n",
      "Batch: 481, Loss: 0.199853, TestLoss: 0.197966, Acc: 0.940594\n",
      "Batch: 482, Loss: 0.247807, TestLoss: 0.194268, Acc: 0.940594\n",
      "Batch: 483, Loss: 1.286255, TestLoss: 0.204478, Acc: 0.925743\n",
      "Batch: 484, Loss: 0.020846, TestLoss: 0.192872, Acc: 0.945545\n",
      "Batch: 485, Loss: 0.070590, TestLoss: 0.198674, Acc: 0.935644\n",
      "Batch: 486, Loss: 0.002256, TestLoss: 0.185304, Acc: 0.945545\n",
      "Batch: 487, Loss: 0.072029, TestLoss: 0.195461, Acc: 0.930693\n",
      "Batch: 488, Loss: 0.003277, TestLoss: 0.199768, Acc: 0.935644\n",
      "Batch: 489, Loss: 0.041883, TestLoss: 0.202663, Acc: 0.930693\n",
      "Batch: 490, Loss: 0.012908, TestLoss: 0.201565, Acc: 0.935644\n",
      "Batch: 491, Loss: 0.411850, TestLoss: 0.200924, Acc: 0.935644\n",
      "Batch: 492, Loss: 0.141190, TestLoss: 0.200010, Acc: 0.940594\n",
      "Batch: 493, Loss: 0.085340, TestLoss: 0.189470, Acc: 0.940594\n",
      "Batch: 494, Loss: 0.020580, TestLoss: 0.192371, Acc: 0.945545\n",
      "Batch: 495, Loss: 0.552466, TestLoss: 0.202668, Acc: 0.925743\n",
      "Batch: 496, Loss: 0.003150, TestLoss: 0.189217, Acc: 0.940594\n",
      "Batch: 497, Loss: 0.006798, TestLoss: 0.187581, Acc: 0.945545\n",
      "Batch: 498, Loss: 0.003138, TestLoss: 0.191355, Acc: 0.940594\n",
      "Batch: 499, Loss: 0.015259, TestLoss: 0.194930, Acc: 0.945545\n"
     ]
    }
   ],
   "source": [
    "MAX_EPOCH = 500\n",
    "\n",
    "for epoch in range(MAX_EPOCH):\n",
    "    net.train()\n",
    "    for i, (data, label) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        pred = net(data)\n",
    "        loss = loss_fn(pred, label)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "    \n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss, correct = 0, 0\n",
    "        for i, (data, label) in enumerate(val_loader):\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            pred = net(data)\n",
    "            val_loss += loss_fn(pred, label).item()\n",
    "            correct += (pred.argmax(1) == label).type(torch.float).sum().item()\n",
    "\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    correct /= len(val_loader.dataset)\n",
    "    print('Batch: {}, Loss: {:>5f}, TestLoss: {:>5f}, Acc: {:>2f}'.format(\n",
    "        epoch, loss.item(), val_loss, correct))\n",
    "    scheduler.step(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, 'model_211206.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_name = ['bad', 'good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_name = dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = torch.load('model_211206.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import image_pyfunc\n",
    "\n",
    "from importlib import reload\n",
    "image_pyfunc = reload(image_pyfunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021/12/05 22:32:26 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.11.0a0+d3722d3) contains a local version label (+d3722d3). MLflow logged a pip requirement for this package as 'torchvision==0.11.0a0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'mlflow-env',\n",
       " 'channels': ['conda-forge'],\n",
       " 'dependencies': ['python=3.9.7',\n",
       "  'pip',\n",
       "  {'pip': ['mlflow',\n",
       "    'torch==1.10.0',\n",
       "    'torchvision==0.11.0a0',\n",
       "    'cloudpickle==2.0.0']}]}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.pytorch.get_default_conda_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bad', 'good']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021/12/05 22:32:27 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.11.0a0+d3722d3) contains a local version label (+d3722d3). MLflow logged a pip requirement for this package as 'torchvision==0.11.0a0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2021/12/05 22:32:28 WARNING mlflow.utils.requirements_utils: Found torchvision version (0.11.0a0+d3722d3) contains a local version label (+d3722d3). MLflow logged a pip requirement for this package as 'torchvision==0.11.0a0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    }
   ],
   "source": [
    "image_pyfunc.save_pytorch_model(net, 'model', (1, 3, 540, 540), cls_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_pytorch_env_patch():\n",
    "#     e = mlflow.pytorch.get_default_conda_env()\n",
    "#     e['channels'].append('pytorch')\n",
    "#     e['dependencies'].extend(['pytorch', 'torchvision', 'torchaudio', 'cudatoolkit=11.3'])\n",
    "#     find_pip = tuple(filter(lambda p: isinstance(p, dict) and 'pip' in p, e['dependencies']))\n",
    "#     find_torch = tuple(filter(lambda p: 'torch' in p, find_pip[0]['pip']))\n",
    "#     for p in find_torch:\n",
    "#         find_pip[0]['pip'].remove(p)\n",
    "#     return e    \n",
    "#     \n",
    "# print(get_pytorch_env_patch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
